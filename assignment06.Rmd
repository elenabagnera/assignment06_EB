---
title: "assignment06"
author: "Elena Bagnera"
date: "10/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(haven)
library(tidymodels)
library(purrr)
library(parsnip)
```

## Question 1

RMSE=
MAE=

Because if the squaring, RMSE penalizes large errors more than MSA does. 
```{r rmse}
knitr::include_graphics("image0 (1).jpeg")

```

## Question 2

```{r confusion}
knitr::include_graphics("image2.jpeg")
```


## Question 3

```{r multiclass}
knitr::include_graphics("image3.jpg")
```

## Question 4

*Consider a population where it is known that 0.49 of observations have a value of 0 and 0.51 of observations
have a value of 1. Approximately what accuracy can be achieved by simply guessing the same value for all
observations? What number should you predict?*

You would get a 51% accuracy if you guess 1 for all observations and 49 if you guess 0. What you want to predict depends on context. Even if guessing 1 leads to more accuracy, getting the observations that have a value of 0 may be very important (e.g. for targeting a program)

*Consider a population where it is known that 0.99 of observations have a value of 0 and 0.01 of observations
have a value of 1. Approximately what accuracy can be achieved by simply guessing the same value for all
observations? What number should you predict?*

You would get a 1% accuracy if you guess 1 for all observations and 99% if you guess 0. What you want to predict depends on context. Even if guessing 0 leads to more accuracy, getting that one observation that has a value of 1 may be very important (e.g. for example if you are trying to predict cancer). You may want to use sensitivity in this case.

*Explain why it is important to consider context when comparing calculated accuracy in different supervised
machine learning tasks?*
Based on your policy goal, positives, true negatives, false positives, and false negatives carry different costs. This is why it's good to consider other measures other than accuracy.

## Question 5
1. Divide the marbles data set into a training set with 80% of observations and a testing set 20% of
observations. Set the seed to 20200229 before sampling.
2. Use count() and library(ggplot2) to develop and justify a intuitive/mental model for predicting
black marbles.
3. Construct a custom function that takes a vector of sizes and returns a vector of predicted colors. Apply
it to the testing data. The R4DS chapter on functions is helpful.
4. Construct a custom function that takes y and y_hat that returns calculated accuracy and a confusion
matrix. Until now, we have only returned one object from a custom function. Use list()
inside of return() to return more than one object. Apply it to the data from part 3. Do not use
yardstick::conf_mat() or caret::confusionMatrix().
5. Using the same testing and training data, estimate a decision tree/CART model with functions from
library(parnsip). Use the “rpart” engine.
6. Does the decision tree/CART model generate the same predictions on the testing data as the model
from part 2? Why or why not?

```{r loading}
set.seed(20200229)

marbles_original <- read_csv("Data/marbles.csv", col_types = cols(col_character()))

# creating a split object
marbles_split <- initial_split(data=marbles_original, prop=0.8)

# create the training and testing data
marbles_train <- training(x = marbles_split)
marbles_test <- testing(x = marbles_split) # putting this to the side

```

We can use a model that predicts black when we have a large sized ball.

```{r model}
marbles_train %>%
  count(color, size) #black balls are mostly big 40/49 of the times 

```

```{r function1}
marbles_train %>%
  
  
#' Title
#'
#' @param .color 
#' @param .size
#'
#' @return
#' @export
#'
#' @examples

 choose_black <- function(x) {
   predict <- predict()
  if (size=="large" {
    prediction= "black"
    } else {prediction="white"
    
    map_chr(.x="size", .f=choose_black) ## does this return a character vector?
    return(map_chr)
  }
   


```

```{r function2}


```


```{r function2}
parsnip::rpart_train(formula = , data = marbles_train, method="class")
plot()

#or

tree <- rpart("black" ~ ., data = marbles_train, control = rpart.control(cp = 0.0001)

```

## Question 6

```{r data}

# input the data
rats <- tribble(
  ~rat_burrow, ~pizza_distance,
  1, 0.01,
  1, 0.05,
  1, 0.08,
  0, 0.1,
  0, 0.12,
  1, 0.2,
  1, 0.3,
  1, 0.5,
  1, 0.75,
  0, 0.9,
  1, 1,
  0, 1.2,
  0, 2.2,
  0, 2.3,
  0, 2.5,
  1, 3,
  0, 3.5,
  0, 4,
  0, 5,
  0, 7
) %>%
  mutate(rat_burrow = factor(rat_burrow))

# split into training and testing data
split <- initial_split(rats, prop = 0.75)

rats_training <- training(split)

rats_testing <- testing(split)

rats_k1 <- vfold_cv(data = rats_training,
                    v = 3)

rats_k3 <- vfold_cv(data = rats_training,
                    v = 3)

rats_kn <- vfold_cv(data = rats_training,
                    v = 3)
```

```{r analysis_data}

resample1_data <- rats_k1$splits[[1]] %>%
  analysis() 


```

```{r analysis_resample1}

resample1_assessment <- rats_k1$splits[[1]] %>% #KNN & k=1
  assessment() 

```

```{r analysis_resample2}


resample1_assessment <- rats_k3$splits[[1]] %>% #KNN & k=2
  assessment() 

```


```{r analysis_resample3}

resample1_assessment <- rats_kn$splits[[1]] %>% #KNN & k=3
  assessment() 

```